Apache Catalyst - это оптимизатор для Apache Spark SQL

Apache Spark может работать с разными языками (SQL, python DataFrame API, Dataset), но в итоге они превращаются в дерево запроса и передается Catalyst, планировщику.

## Архитектура

Архитектура Cascades-like

План работы:
1. Analysis - биндинг объектов на объекты системного каталога
	1. Этап парсинга пропускается, так как на вход уже подается дерево запроса (запрос парсит конкрентный обработчик API)
	2. Статистика собирается здесь
2. Logical optimization - применение правил реляционной алгебры для трансформации логического плана в эквивалентное, Logical->Logical
	1. На этом этапе также происходит перебор JOIN'ов (cost-based)
3. Physical planning - превращение логических выражений в физические планы, Logical->Physical
	1. Также запускаем rule executor для проверки того, что план готов к выполнению. Например, проверка требований (requirement, аналог property из Cascades)
4. Costing - подсчет стоимости физических планов
5. Codegen - для полученного плана запроса генерируется JVM байткод

## План

План бывает 2 типов:
1. Логический - операция, которую необходимо выполнить, без деталей реализации. Содержит:
	1. Output - что возвращает
	2. Constraints - инвариант, создаваемых кортежей (грубо говоря, предикаты на нем)
	3. Statistics - статистика (как по всему оператору, так и по каждому столбцу)
2. Физический - конкретная реализация оператора

## Transformations

Transformation - правило, изменяющее дерево запроса/выражения.

Базовый блок для правил:
- Optimization: Logical->Logical
- Strategy: Logical->Physical
- Preparation: Physical->Physical

Реализуются с помощью механизмов Scala паттерн матчинга и частичных функций (каррирование).

Многие правила реализуются с помощью комбинирования разных базовых правил.

Правила сгруппированы в батчи (Batch). Внутри батча правила выполняются последовательно. Можно добавить различные зависимости между правилами.

Получается такая иерархия:
1. Batch 1
	1. Rule 1
		1. Transformation 1
		2. Transformation 2
	2. Rule 2
		1. Transformation 1
	3. Rule 3
		1. Transformation 1
		2. Transformation 2
		3. Transformation 3
2. Batch 2
	1. Rule 1
		1. Transformation 1
		2. Transformation 2
3. Batch 3
	1. Rule 1
		1. Transformation 1
		2. Transformation 2
	2. Rule 2
		1. Transformation 1

У правил есть 2 стратегии выполнения:
1. Once - выполняем единожды
2. Fixed point - выполняем до определенного условия (например, пока дерево выражения не станет определенного вида)

## Adaptive Query Execution

Apache Spark работает с распределенными данными, хранящимися в различных СУБД, поэтому статистика может быть уже не актуальна.

Решение - Adaptive Query Execution, изменение плана выполнения во время выполнения на основании собранной статистики (runtime statistics).

Cost-based optimization работает плохо когда:
1. Статистика устарела
2. Сбор статистики дорогой
3. В предикатах находятся UDF
4. Хинты не работают для постоянно изменяющихся данных

План работы AQE:
1. План выполнения делится на Query stage
2. В конце каждого Query stage добавляется SHUFFLE узел
3. После него происходит дополнительная оценка статистики и возможное изменение плана выполнения

Замечания:
1. SHUFFLE добавляют в конец pipeline breaker: AGGREGATE/SORT и т.д.
2. После очередного Query Stage план выполнения может измениться сильно
3. Полностью планировщик не запускается - только его часть, которая даст части Query stage, а потом дополняем план выполнения.

## Полезные идеи

1. Группировка правил в батчи и назначение стратегий выполнения. Может быть полезно для декорреляции запросов.

## Источники
1. [A Deep Dive into Spark SQL's Catalyst Optimizer (Cheng Lian + Maryann Xue, DataBricks)](https://www.youtube.com/watch?v=Xb2zm4-F1HI)